---
title: Modern Connectivity Whitepaper
date: 11. February 2019
layout: page
---

# High-Availability Secure Networking for Media Companies

## The Need for Speed (and Security)

The revolution is over. Digital acquisition and production of media is here. [Moore's law](https://en.wikipedia.org/wiki/Moore%27s_law) has held true for the past 45+ years. A radical explosion in the speed of electronic information processing has set in motion the most fundamental shifts in the way we process information since the introduction of mechanical movable type printing to western culture by Johannes Gutenberg in ca. 1450. In the field of media production, this digital revolution has brought digital film cameras and audio recording devices that can match almost all analog recording formats in terms of quality, and certainly in the availability of storage quantity. With the digital acquisition of video and audio sources comes the digital distribution of source, intermediate and master material via computer networks.

This reality forces production and postproduction providers to invest in high-performance network infrastructure in their [LAN](https://en.wikipedia.org/wiki/Local_area_network): 10Gbps or even 40Gbps Ethernet over dedicated fibre-optical connections to high-capacity storage solutions are becoming necessary with the increase of both spatial- as well as color-resolution beyond FullHD and 8bit. 4K and larger sources in 10bit, 12bit and beyond are necessary to deliver the quality necessary for high-grade finishing. Such systems generally called "Storage Area Networks" - SANs in short - rely on managed networks provided by business-grade hardware. Consumer routers as provided by Internet Service Providers or switches bought cheap at your local electronics retailer have no place in these networks. Neither do Cat. 5 cables suffice to deliver the throughput you need to keep the work going in your suites.

A crucial requirement that mustn't be overlooked is that of security. With the advantages of server-based post workflows comes the risk of a breach of network security. With all of your production's expensive assets on a series of networked servers, these assets could potentially become accessible from without your LAN. If an intruder gains access to your network through the internet, they may steal, leak or delete your assets, potentially causing significant damage to your business. You may have to restore data from a backup - if there is one, may be liable to pay damages and loose current or future work. It is therefore imperative to plan for adequate network security, even in small post houses or one-man shows when NAS / SAN technology is used.

One aspect so far overlooked is your connection to wide-area networks. While local high-speed connectivity is one pillar of a functioning postproduction, the speed with which you can access wide-area networks (WAN) - e.g. the internet - can severely limit your ability to download assets, upload deliverables to your clients or run backups or archive requests to a cloud. This can foreseeably even be the determining factor in how you must time and offer services, if they require large up- or downloads. Also, in the world of IP based live video production, adequate bandwidth is quintessential to deliver your product.

In this whitepaper I will outline requirements and solutions for these three aspects:
1) high-speed local connectivity for 4K and above content
2) maintaining accessability and ease of use while ensuring security of assets
3) options to maximize internet bandwidth

## Maximizing Local Bandwidth

### Good Physical Connection

To facilitate high-speed connectivity in your local network, a variety of options are available ranging from simple "change the cable" to a more involved process of buying dedicated equipment. The first thing ensure is quality cabling. While conventional copper-wiring is often already available in office spaces, just because it has an [RJ-45](https://en.wikipedia.org/wiki/Modular_connector#8P8C) connector doesn't mean it can deliver the speed you want. Adequate cables, terminators and wall sockets are crucial.

The household-standard network standard, Ethernet, exists since the 1980s and for copper-wired networks, speeds have increased one-thousand-fold in this timeframe, growing from in bandwidth from 10 Megabits per second to 10 Gigabits per second (10Mb/s to 10Gb/s). To put this in perspective for media companies, today, you can theoretically push about 4.5 TeraBytes through a 10Gb-Ethernet connection per hour. For most non-backbone connections, meaning workstations in postproduction suites or on set, that is usually sufficient. Due to the long history of Ethernet, however, if you plug new 10GbE hardware into an old wall socket that is wired with copper cables conforming to the Cat.5E or lower standard, you will not get the speed you desire. Even Cat.6 and above cables that theoretically support 10GbE might fall short if they are bent to sharply, twisted or the shielding has ben cut. This might result in less than ideal data rates on those connections.

In general, it pays to have an expert install cabling to match the speed you wish to achieve. Cat.6 and Cat.6A (augmented) are adequate for patch-cable distances (connecting devices within a 19-inch rack, or bridging the distance from a wall socket to your computer), while the more sturdy Cat.7 cables are more suited for installation in walls or over larger distances. That is only for copper wires, though.

The go-to way to connect devices to facilitate the next jump in connection speed by an order of magnitude is fibre-optic connections. These lightweight, flexible cables carry pulses of light instead of electric current and are already common in many network applications in the form of the [Small Form-Factor Pluggable](https://en.wikipedia.org/wiki/Small_form-factor_pluggable_transceiver#SFP+) - SFP for short. In the enhanced version - SFP+ - they carry up to 10Gbps in an Ethernet network, but the deployment of new form-factors for 40GbE and 100GbE is already under way. In all fairness, even copper wired cables can carry 40GbE in certain configurations.

That is all good and well, though the real question is, how much bandwidth does a postproduction facility require to operate without being disturbed by lagging connections? TB/h is a great metric  if you want to calculate offload times in a data-wrangling situation, yet doesn't say much in terms of how much video editing you can do at once.

- throttel certain routes
- install a dedicated SAN, maybe even fibre

### Storage Bandwidth Requirements in Digital Post-Production & Data Wrangling

The bandwidth with which computers connect to storage is best thought of in terms of video streams. As such the bandwidth requirement of a stream can in simple terms be considered as the bitrate of the codec. Say you have an [Apple ProRes 4444 [PDF]](https://www.apple.com/final-cut-pro/docs/Apple_ProRes_White_Paper.pdf) file in 1920x1080p25, then your data rate is approximately 300 Mbit/s. Clearly, even a 1 GbE connection can provide up to three such streams (as needed in a multicam scenario, for example). This theoretical calculation assumes a great many things, though. It considers a single workstation connected via a 1GbE port is reading a single file from a single storage device connected to a network via a 1GbE connection with no other factors considered.

The reality of a postproduction facility with multiple workstations is quite different. Multiple clients will be reading and writing multiple files to a variety of storage devices. Such a facility will have an online storage solution like an Avid Nexis, TeraBlock or EditShare server, should have a backup "nearline" storage and might have a workstation writing a tape for archival or off-site backup purposes. Suddenly, the demand on the network is significantly higher.

Imagine this network as a series of pipes. All the workstations are connected with pipes of equal bandwidth: capacity per time interval. All these pipes are 10GbE connections that all run into a switch, where a large pipe collects all the water - or data - running into it and distributes it to the network-attached storage via yet another pipe. If we have 10 workstations pushing at full bandwidth, the switch will have to handle 100 Gb/s from the ten 10Gb/s pipes (again: ideal figures ignoring overhead etc.). If all this data is to run into the server, the pipe between switch and server also would have to be 100Gb/s. This is a crucial bottleneck to think about. The NAS must be connected at multiple times the bandwidth of the various clients, or the pipes will not be able to let enough water through, causing a information to be queued for transfer. This becomes lag in applications which an operator will experience and might have to justify to a client.

To adequately design a computer network to handle the demands of digital film post-production, the first question to ask is the required number and bandwidth of simultaneous video streams that will have to be read from the storage solution. A simple offline edit in 1920x1080 without need for multicam editing might get by with just a few streams. When the resolution switches to 4K, all of a sudden, the bandwidth is increased by a factor of four (theoretically). If you run a compositing application like Nuke or AfterEffects, you might be pulling from dozens of video sources at once, some might be 8K or above. Or maybe you are color grading just a few 8K raw files. Either way, these demands have to be figured into the choice of switch and connection to the storage solution. It is crucial to understand that not all switches are created equal in terms of the internal bandwidth they provide and cheap consumer products often don't provide specifications to evaluate the product. Invariably, professional products should be chosen over ISP-provided hardware.

### The Mare of Direct-Attached Storage

With the reference speeds established above, quick work can be made of evaluating local storage media such as external hard-drives or solid-state drives. It doesn't matter whether using *Just a Bunch of Disks* - JOBDs - or a *Redundant-Array of Independent Disks* - a RAID, locally connecting them means using either the household-standard USB, Thunderbolt or maybe an external SATA port. None of these, by specification,  in a single-cable configuration, pass 10Gb/s of bandwidth. This is assuming that the storage medium itself, the harddisk or SSD are fast enough to saturate the connection. Depending on USB type (3.1 vs 3.0 vs 2.0) and the Thunderbolt type, these connections can be drastically slower. Also, with USB, the internal bus layout of the computer can dramatically reduce bandwidth. Simply, most Direct-Attached-Storage is slower than modern professional Network-Attached solutions based on the architecture of the connections.

Beyond the technical limitations, DAS solutions are also less secure: hard-drives can be stolen off desks, they can be dropped, a single drive could just fail, incurring significant recovery costs. A server room can be locked and the server backed up. Working on a local drive means a backup solution must be in place at every workstation to ensure quick and effortless failover. Finally, the administration of a multi-suite postproduction workflow with portable storage media is highly inefficient and redundant, since many copies of raw footage have to be created, hard-drives moved and copied over.

On a TV series project for a large German broadcaster in 2018 the estimate for required number of 1TB portable hard-drives was six-thousand. Given 120TB of footage simultaneously available to eight editors with backup and redundancy, an entire office would have been stacked ceiling-high with hard-drives. The server-solution finally deployed was significantly cheaper, faster, more reliable, highly redundant and easier to administer. In short: unless you are working on just one project on one workstation, there is no viable reason to use direct-attached storage. And even then, a networked solution holds advantages.

Tape storage such as [Linear Tape-Open](https://en.wikipedia.org/wiki/Linear_Tape-Open) has been excluded from this analysis, since, while they are amazingly durable and cost-effective, 

# WAN Access

The first thing you should do when upgrading your network is this: get a professional router. Almost everything about the routers distributed by Internet Service Providers is bad. The wifi antennas are most likely small, the routing options are limited, firewall options are ridiculous, the internal switching bandwith is small. It might even be throtteling certain ports, for example when you use Voice-Over-IP. It is a dangerous, slow piece of junk that has no place in adminstering a high-performance, secure network. If you can get rid of it entirely by getting a router that can connect to your DSL, T1 or Fibre Optics connection, do that. If you cannot get around it, use it strictly as an intermediary between your provider and your new, shiny, professional router. From now on, this new router will take care of DHCP, Wifi, VPN, firewalls, throtteling, subnets and so forth.

- bonding must be accessible for all protocols!
